:opencf: link:https://labs.opentlc.com/[OPENTLC lab portal^]
:account_management: link:https://www.opentlc.com/account/[OPENTLC Account Management page^]
:quay_hostname: quay.rhte.example.opentlc.com
:cluster_base: REPL.rhte.opentlc.com
:cluster_master: master.{cluster_base}
:bastion: bastion.{cluster_base}
:kibana: https://kibana.apps.{cluster_base}
:application1: app1
:application2: app2
:oc_version: 3.10.14
:oc_download_location: https://mirror.openshift.com/pub/openshift-v3/clients/{oc_version}

= Logging OpenShift Lab

== Lab Overview

Welcome to the RHTE Logging for Hybrid Cloud lab. This lab is based on the Aggregated Logging feature available on OpenShift Container Platform (OCP). This Aggregated Logging is based on the popular Elasticsearch, Fluentd, Kibana (EFK) stack.

The Aggregated Logging feature aggregates node (system), Kubernetes events, and pod logs and saves it for search, analysis and dashboards.

In the lab, expect to learn how to install, and upgrade the aggregated Logging Stack and use the powerful features of Kibana to visualize logs for troubleshooting and for dashboards.

== Lab Setup

=== Configure the OpenShift Command Line Interface

In this lab, you use the command line for some of the tasks. The lab assumes that you are using your laptop and a local OpenShift command line utility.

You will need version {oc_version} of the OpenShift command line utility. If you already have the OpenShift CLI installed you can check the version installed:

[source,bash]
----
oc version
----

.Sample Output
[source,text]
----
oc v3.10.14
kubernetes v1.10.0+b81c8f8
features: Basic-Auth
----

Make sure the the version displayed matches {oc_version}.

If it does not you can download the correct version of the OpenShift Command Line interface from {oc_download_location}. In this directory you will find a `linux`, `windows` and `macosx` subdirectory. Navigate to the directory for your operating system and download the OpenShift command line utility (oc.tar.gz for Linux and MacOS or oc.zip for Windows). You will need need to unarchive the downloaded file and then move the "oc" binary into a directory in your PATH (usually $HOME/bin).

Then verify again that the version displayed is corect.


=== These are your lab end points


cluster_base: {cluster_base}

cluster_master: {master_base}

bastion: {bastion}

kibana: {kibana}

Where REPL = Lab Attendees GUID

=== Connect to the OpenShift Cluster via oc


Make sure you can log in to the OpenShift cluster.

[NOTE]
The cluster URL is https://{cluster_master}.

. Use the `oc login` command to log in to the cluster, making sure to replace the URLs with your cluster's URLs:
+
[source,text]
----
oc login -u user1 -p r3dh4t1! <CLUSTER_URL>
----
+
[NOTE]
If you see a warning about an insecure certificate, type `y` to connect insecurely.
+
.Sample Output
[source,text]
----
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>
----

=== Connect to the OpenShift Web Console

Make sure you can log in to the OpenShift cluster as user2.

[NOTE]
The cluster URL is https://{cluster_master}.

[NOTE]
We recommend that you use "incognito" or "private browsing" mode of your browser, since we will use several users and we don't want to mix up the cookies.

. Use your web browser to log in to the cluster https://{cluster_master}.
. At the login prompt use username `user1` and password `r3dh4t1!`

=== Connect to the bastion host via ssh

Connect to the {bastion} host via SSH using the private key for OpenTLC:
[source,bash]
----
ssh -i <Your OPENTLC private key> <Your OPENTLC UserID>@<BASTION_HOST>
----

Once you're on the bastion host, become root:
[source,bash]
----
sudo -i
----

Check that you're `system:admin` on your OpenShift cluster:
[source,bash]
----
oc whoami
----

You should see

.Output
[source,text]
----
system:admin
----

OpenShift cluster has pre-created users. We will be using `user1` and `user2`
Add `cluster-admin` privileges to the `user2`:
[source,bash]
----
oc adm policy add-cluster-role-to-user cluster-admin user2
----
You should see

.Output
[source,text]
----
cluster role "cluster-admin" added: "user2"
----

== Create applications in the OpenShift Cluster

In this section you are creating an applications in the cluster. The application will run in user's namespaces and produce various logs. In the real world this might be end-user's application.

=== Create application as user1 via command line

. Log into the OpenShift Cluster under user1 (use https://{cluster_master} as `<CLUSTER_URL>`).
+
[source,bash]
----
oc login -u user1 -p r3dh4t1! <CLUSTER_URL>
----
+
. Create a project for Jenkins
+
[source,bash]
----
oc new-project lab1 --display-name "First Log producer"
----
+
. You will be building a (very) simple python application.



=== Check that the application logs are flowing in

In order to view application logs you will need to log in to Kibana as a regular user.

. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user1` and password `r3dh4t1!`
+
[NOTE]
If you already logged in as a different user you may want to use another browser or incognito browsing window.
+
. Open `Settings` -> `About`. Kibana version should be: `4.6.4`
. In the `Discover` tab of Kibana add columns for:
.. `kubernetes.namespace_name`
.. `kubernetes.pod_name`
.. `message`
. Verify that logs from the application deployed as `user1` can be seen and no logs from namespace created as `user2` can be seen

Validated at this point:
Regular user can only see logs from his/her own application.

=== Check that the system logs are flowing in

In order to view system logs you should log in to Kibana as a user with cluster-admin privileges:

. Close the old incognito browsing window, so that old cookies don't interfere with the new login.
. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user2` and password `r3dh4t1!`
+
[NOTE]
user2 was given cluster-admin privileges previously.
+
. In the `Discovery` tab of Kibana switch to `project.*` index pattern
. Then add columns for:
.. `kubernetes.namespace_name`
.. `kubernetes.pod_name`
.. `message`
. Verify that logs from the application deployed as `user1` can be seen as well as logs from namespace created as `user2`
. In the `Discovery` tab of Kibana switch to `.operations.*` index pattern
. Verify that system logs from hosts can be seen

Validated at this point:
Cluster admins can see logs from all namespaces
Cluster admins can see logs from hosts and administrative namespaces

== Upgrade the Logging setup

In this section you will upgrade the logging bits to logging from OpenShift version 3.11.

* SSH to the bastion host.
* Become root on the bastion host
[source,bash]
----
sudo -i
----
* Check out the repository that contains logging bits from OKD 3.11 (snapshot of openshift-ansible repo):
[source,bash]
----
cd /root/
git clone https://github.com/t0ffel/openshift-ansible
----
* Edit the inventory file:
[source,bash]
----
vim /etc/ansible/hosts
----
* Add the following lines to the inventory file in `[OSEv3:vars]` section right after `Enable cluster logging`.
[source,bash]
----
openshift_logging_image_prefix="openshift/origin-"
openshift_logging_elasticsearch_image="openshift/origin-logging-elasticsearch5"
openshift_logging_kibana_image="openshift/origin-logging-kibana5"
openshift_logging_fluentd_image="openshift/origin-logging-fluentd"
openshift_logging_es_allow_external=True
openshift_logging_elasticsearch_proxy_image=openshift/oauth-proxy:v1.1.0
openshift_logging_kibana_proxy_image=openshift/oauth-proxy:v1.1.0
----
This directs the installer to use OKD images of logging components from v3.11 and allow external route to Elasticsearch cluster.

Enable cluster logging section should look as follows:
[source,bash]
----
# Enable cluster logging
########################
openshift_logging_install_logging=True
openshift_logging_image_prefix="openshift/origin-"
openshift_logging_elasticsearch_image="openshift/origin-logging-elasticsearch5"
openshift_logging_kibana_image="openshift/origin-logging-kibana5"
openshift_logging_fluentd_image="openshift/origin-logging-fluentd"
openshift_logging_es_allow_external=True
openshift_logging_elasticsearch_proxy_image=openshift/oauth-proxy:v1.1.0
openshift_logging_kibana_proxy_image=openshift/oauth-proxy:v1.1.0

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/srv/nfs
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
openshift_logging_storage_labels={'storage': 'logging'}
openshift_logging_es_pvc_storage_class_name=''

openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"}

openshift_logging_es_cluster_size=1
openshift_logging_curator_default_days=3
----
* Make sure you're in the `/root/openshift-ansible` directory:
[source, bash]
----
cd /root/openshift-ansible
----
* Execute ansible-playbook to upgrade the logging bits:
[source,bash]
----
ansible-playbook -vvv --become --become-user root \
  --inventory /etc/ansible/hosts \
  -e deployment_type=origin \
  -e openshift_image_tag=v3.11 \
  -e openshift_pkg_version="-3.11*" \
  playbooks/openshift-logging/config.yml
----

[Note]
Execution of the playbook may take few minutes.

This instructs the installer to deploy logging bits from v3.11

Ansible playbook should finish running without any errors.

.Sample Output
[source,text]
----
PLAY RECAP *************************************************************************************************************************************************************************
infranode1.4b43.internal   : ok=2    changed=1    unreachable=0    failed=0
localhost                  : ok=12   changed=0    unreachable=0    failed=0
master1.4b43.internal      : ok=303  changed=58   unreachable=0    failed=0
node1.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
node2.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
support1.4b43.internal     : ok=1    changed=0    unreachable=0    failed=0


INSTALLER STATUS *******************************************************************************************************************************************************************
Initialization   : Complete (0:00:10)
Logging Install  : Complete (0:02:58)
----

Validate that the logging stack was updated:

. Navigate to `openshift-logging` namespace and check the running pods
[source,bash]
----
oc project openshift-logging
oc get po
----

.Sample Output
[source, text]
----
[root@bastion openshift-ansible]# oc project openshift-logging
Now using project "openshift-logging" on server "https://master.4b43.openshift.opentlc.com:443".
[root@bastion openshift-ansible]# oc get po
NAME                                      READY     STATUS    RESTARTS   AGE
logging-es-data-master-7m1ej4aj-6-kppvt   2/2       Running   0          1m
logging-fluentd-5xdsq                     1/1       Running   0          1m
logging-fluentd-65hwq                     1/1       Running   0          1m
logging-fluentd-9bndh                     1/1       Running   0          1m
logging-fluentd-wf6hc                     1/1       Running   0          3h
logging-kibana-2-mnhm7                    2/2       Running   0          1m
[root@bastion openshift-ansible]#
----
Age of the Elasticsearch pod should be new - it was just spun up by the installer.

It should be using image `openshift/origin-logging-elasticsearch5`. You can check it for example if you do `oc describe pod <POD NAME>`

.Sample Output
[source, text]
----
[root@bastion openshift-ansible]# oc describe po logging-es-data-master-7m1ej4aj-6-kppvt
Name:           logging-es-data-master-7m1ej4aj-6-kppvt
Namespace:      openshift-logging
Node:           infranode1.4b43.internal/192.199.0.91
Start Time:     Fri, 24 Aug 2018 17:26:35 +0000
Labels:         component=es
                deployment=logging-es-data-master-7m1ej4aj-6
                deploymentconfig=logging-es-data-master-7m1ej4aj
                logging-infra=elasticsearch
                provider=openshift
Annotations:    openshift.io/deployment-config.latest-version=6
                openshift.io/deployment-config.name=logging-es-data-master-7m1ej4aj
                openshift.io/deployment.name=logging-es-data-master-7m1ej4aj-6
                openshift.io/scc=restricted
Status:         Running
IP:             10.1.2.38
Controlled By:  ReplicationController/logging-es-data-master-7m1ej4aj-6
Containers:
  elasticsearch:
    Container ID:   docker://8fee48f81997355bb03dea77d692605d367c799b25bdb950ce4fdacfa4bac453
    Image:          openshift/origin-logging-elasticsearch5
    Image ID:       docker-pullable://docker.io/openshift/origin-logging-elasticsearch5@sha256:5151753acbc1d55459223cda392cead1adfb53c6d9e4955b39d4cca5536faa58
    Ports:          9200/TCP, 9300/TCP
    Host Ports:     0/TCP, 0/TCP
    State:          Running
      Started:      Fri, 24 Aug 2018 17:26:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  16Gi
    Requests:
      cpu:      1
      memory:   16Gi
    Readiness:  exec [/usr/share/elasticsearch/probe/readiness.sh] delay=10s timeout=30s period=5s #success=1 #failure=3
    Environment:
      DC_NAME:                  logging-es-data-master-7m1ej4aj
      NAMESPACE:                openshift-logging (v1:metadata.namespace)
      KUBERNETES_TRUST_CERT:    true
      SERVICE_DNS:              logging-es-cluster
      CLUSTER_NAME:             logging-es
      INSTANCE_RAM:             16Gi
      HEAP_DUMP_LOCATION:       /elasticsearch/persistent/heapdump.hprof
      NODE_QUORUM:              1
      RECOVER_EXPECTED_NODES:   1
      RECOVER_AFTER_TIME:       5m
      READINESS_PROBE_TIMEOUT:  30
      POD_LABEL:                component=es
      IS_MASTER:                true
      HAS_DATA:                 true
      PROMETHEUS_USER:          system:serviceaccount:openshift-metrics:prometheus
      PRIMARY_SHARDS:           1
      REPLICA_SHARDS:           0
    Mounts:
      /elasticsearch/persistent from elasticsearch-storage (rw)
      /etc/elasticsearch/secret from elasticsearch (ro)
      /etc/podinfo from podinfo (ro)
      /usr/share/java/elasticsearch/config from elasticsearch-config (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from aggregated-logging-elasticsearch-token-q7v4t (ro)
  proxy:
    Container ID:  docker://cdb5626aab755615c2dfd2b7287cb266d7d4ef6f8c09e663127ea188f73c4830
    Image:         openshift/oauth-proxy:v1.1.0
    Image ID:      docker-pullable://docker.io/openshift/oauth-proxy@sha256:731c1fdad1de4bf68ae9eece5e99519f063fd8d9990da312082b4c995c4e4e33
    Port:          4443/TCP
    Host Port:     0/TCP
    Args:
      --upstream-ca=/etc/elasticsearch/secret/admin-ca
      --https-address=:4443
      -provider=openshift
      -client-id=system:serviceaccount:openshift-logging:aggregated-logging-elasticsearch
      -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
      -cookie-secret=ekdMM0pXbkJwbExTOEJHQg==
      -basic-auth-password=66l71kZ55ccghoTC
      -upstream=https://localhost:9200
      -openshift-sar={"namespace": "openshift-logging", "verb": "view", "resource": "prometheus", "group": "metrics.openshift.io"}
      -openshift-delegate-urls={"/": {"resource": "prometheus", "verb": "view", "group": "metrics.openshift.io", "namespace": "openshift-logging"}}
      --tls-cert=/etc/tls/private/tls.crt
      --tls-key=/etc/tls/private/tls.key
      -pass-access-token
      -pass-user-headers
    State:          Running
      Started:      Fri, 24 Aug 2018 17:26:37 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  64Mi
    Requests:
      cpu:        100m
      memory:     64Mi
    Environment:  <none>
    Mounts:
      /etc/elasticsearch/secret from elasticsearch (ro)
      /etc/tls/private from proxy-tls (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from aggregated-logging-elasticsearch-token-q7v4t (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  proxy-tls:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  prometheus-tls
    Optional:    false
  elasticsearch:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  logging-elasticsearch
    Optional:    false
  elasticsearch-config:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      logging-elasticsearch
    Optional:  false
  podinfo:
    Type:  DownwardAPI (a volume populated by information about the pod)
    Items:
      limits.memory -> mem_limit
  elasticsearch-storage:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  logging-es-0
    ReadOnly:   false
  aggregated-logging-elasticsearch-token-q7v4t:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  aggregated-logging-elasticsearch-token-q7v4t
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  node-role.kubernetes.io/infra=true
Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type    Reason     Age   From                               Message
  ----    ------     ----  ----                               -------
  Normal  Scheduled  2m    default-scheduler                  Successfully assigned logging-es-data-master-7m1ej4aj-6-kppvt to infranode1.4b43.internal
  Normal  Pulled     2m    kubelet, infranode1.4b43.internal  Container image "openshift/origin-logging-elasticsearch5" already present on machine
  Normal  Created    2m    kubelet, infranode1.4b43.internal  Created container
  Normal  Started    2m    kubelet, infranode1.4b43.internal  Started container
  Normal  Pulled     2m    kubelet, infranode1.4b43.internal  Container image "openshift/oauth-proxy:v1.1.0" already present on machine
  Normal  Created    2m    kubelet, infranode1.4b43.internal  Created container
  Normal  Started    2m    kubelet, infranode1.4b43.internal  Started container
----


== Create Kibana Dashboards

. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user2` and password `r3dh4t1!`
+
[NOTE]
user2 was given cluster-admin privileges previously.
+
. Open `Management` tab. It should say that the Kibana version is `Version: 5.6.10`


=== Create Saved Searches

. In the `Discovery` tab of Kibana switch to `project.*` index pattern
. Then add columns for:
.. `kubernetes.namespace_name`
.. `kubernetes.pod_name`
.. `message`
. Click `Save` and save the search under name `pod logs`
. In the `Discovery` tab of Kibana switch to `.operations.*` index pattern
. Remove all existing columns and add columns for:
.. `hostname`
.. `systemd.t.COMM`
.. `message`
. Click `Save`
+
[NOTE]
The `Save` button is in right upper corner of Kibana Web UI
+
.. Enter name `ops logs`
.. Check checkbox 'Save as a new search'
.. Click `Save`

=== Create Visualizations
Fist visualization we're going to create is the pie chart visualization of Namespace by volume of logs.

. Click Visualize tab
.. Click `+`
.. Choose `Pie`
.. In `From a New Search, Select Index` select `project.*`
.. Under `buckets` -> `Select buckets type` choose `Split slices`
.. Under `Aggregation` select `Term` in drop-down box
.. Under `Field` scroll down and select `kubernetes.namespace_name`
+
[NOTE]
Once you place the cursor under `Field` you can start typing to find the field name.
+
.. Hit 'run' triangle button to see the preview
.. Click `Save`
.. Enter name `namespaces by log volume`
.. Click `Save`

Next visualization we're going to create is a histogram for ops logs/hosts.

. Click Visualize tab
.. Click `+`
.. Choose `Vertical Bar`
.. In `Or, From a Saved Search` select `ops logs`
+
[NOTE]
We're re-using `ops logs` saved search which we created on an earlier step.
+
.. Under `buckets` -> `Select buckets type` choose `X-Axis`
.. Under `Aggregation` select `Date Histogram` in drop-down box
.. hit 'run' button to preview the histogram
.. Click `Add sub-buckets` below `buckets`->`X-Axis`
.. Under `Select buckets type` choose `Split series`
.. In `Sub Aggregation` drop-down select `Terms`
.. In `Field` choose `hostname`
.. hit 'run' button to preview the histogram
.. Click `Save`
.. Enter name `ops logs histogram by hostname`
.. Click `Save`

Next visualization we will append the `namespaces by log volume` Pie chart to include pod names.

. Click Visualize tab
. Click `namespace by log volume` visualization to open it for editing
. Under `buckets` click `Add sub-bucket`
. In `Select bucket type` choose `Split Slices`
. In `Sub Aggregation` drop-down select `Terms`
. In `Field` choose `kubenetes.pod_name`
. hit 'run' button to preview the visualization
. Check that visualization displays logs volume by pod name and by namespace
. Click `Save`
. Use the same name (`namespaces by log volume`)
. Click `Save`


=== Create Dashboard
We will create a dashboard that will display all the visualizations and saved searches created on previous steps.

. Click Dashboard tab
.. Click `+`
.. Click `add`
.. On the `Visualization` tab select the two visualizations you created previously
.. On the `Search` tab select the two saved searches you saved previously
.. Drag and resize visualizations and searches so that they fit the screen nicely.
.. Click `Save`
.. Enter name `Main dashboard`
.. Click `Save`

== Eventrouter & Timelion

=== Deploy Eventrouter component

Eventrouter is the component of Logging stack that allows collecting Kubernetes events.

* SSH to the bastion host.
* Become root on the bastion host
[source,bash]
----
sudo -i
----
* Edit the inventory file:
[source,bash]
----
vim /etc/ansible/hosts
----
* Add the following lines to the inventory file in `[OSEv3:vars]` section right after `Enable cluster logging`.
[source,bash]
----
openshift_logging_install_eventrouter=True
openshift_logging_eventrouter_image="openshift/origin-logging-eventrouter"
----
This directs the installer to add OKD's eventrouter logging component from v3.11.

Enable cluster logging section should look as follows:
[source,bash]
----
# Enable cluster logging
########################
openshift_logging_install_eventrouter=True
openshift_logging_eventrouter_image="openshift/origin-logging-eventrouter"

openshift_logging_image_prefix="openshift/origin-"
openshift_logging_elasticsearch_image="openshift/origin-logging-elasticsearch5"
openshift_logging_kibana_image="openshift/origin-logging-kibana5"
openshift_logging_fluentd_image="openshift/origin-logging-fluentd"
openshift_logging_es_allow_external=True
openshift_logging_elasticsearch_proxy_image=openshift/oauth-proxy:v1.1.0
openshift_logging_kibana_proxy_image=openshift/oauth-proxy:v1.1.0

openshift_logging_install_logging=True

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/srv/nfs
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
openshift_logging_storage_labels={'storage': 'logging'}
openshift_logging_es_pvc_storage_class_name=''

openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra": "true"}
openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"}

openshift_logging_es_cluster_size=1
openshift_logging_curator_default_days=3
----

* Make sure you're in the `/root/openshift-ansible` directory:
[source, bash]
----
cd /root/openshift-ansible
----
* Execute ansible-playbook to deploy the Eventrouter component:
[source,bash]
----
ansible-playbook -vvv --become --become-user root \
  --inventory /etc/ansible/hosts \
  -e deployment_type=origin \
  -e openshift_image_tag=v3.11 \
  -e openshift_pkg_version="-3.11*" \
  playbooks/openshift-logging/config.yml
----

[Note]
Execution of the playbook may take few minutes.

This instructs the installer to deploy Eventrouter component from OKD v3.11

Ansible playbook should finish running without any errors.

.Sample Output
[source,text]
----
PLAY RECAP *************************************************************************************************************************************************************************
infranode1.4b43.internal   : ok=2    changed=1    unreachable=0    failed=0
localhost                  : ok=12   changed=0    unreachable=0    failed=0
master1.4b43.internal      : ok=303  changed=58   unreachable=0    failed=0
node1.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
node2.4b43.internal        : ok=1    changed=0    unreachable=0    failed=0
support1.4b43.internal     : ok=1    changed=0    unreachable=0    failed=0


INSTALLER STATUS *******************************************************************************************************************************************************************
Initialization   : Complete (0:00:10)
Logging Install  : Complete (0:02:58)
----

Validate that the Eventrouter component was deployed successfully:

. Navigate to `default` namespace and check the running pods
[source,bash]
----
oc project default
oc get po
----

[NOTE]
Eventrouter is deployed to `default` namespace, not to `openshift-logging` namespace.

.Sample Output
[source, text]
----
[root@bastion openshift-ansible]# oc project default
Now using project "default" on server "https://master.4b43.openshift.opentlc.com:443".
[root@bastion openshift-ansible]# oc get po
NAME                          READY     STATUS    RESTARTS   AGE
docker-registry-1-tb69x       1/1       Running   2          17h
logging-eventrouter-1-b9qh2   1/1       Running   0          10m
registry-console-1-hp8x5      1/1       Running   1          17h
router-2-t2f27                1/1       Running   1          17h
[root@bastion openshift-ansible]#
----
Age of the Eventrouter pod should be new - it was just spun up by the installer.

It should be using image `openshift/origin-logging-eventrouter`. You can check it for example if you do `oc describe pod <POD NAME>`

=== View Kuberentes events in Kibana

In this section you will learn how to search for Kubernetes events in Kibana.

. In your web browser open {kibana} in a new incognito window
. When prompted to log in use username `user2` and password `r3dh4t1!`
. In the `Discovery` tab of Kibana switch to `.operations.*` index pattern
+
[NOTE]
You can switch to a different index by selecting it in left upper corner
+
. Remove all existing columns and add columns for:
.. `kubernetes.event.type`
+
[NOTE]
This field shows the type of the event: either `Normal` or `Warning`
+
.. `kubernetes.event.involvedObject.namespace`
+
[NOTE]
This field shows the namespace where the event occurred
+
.. `kubernetes.event.reason`
+
[NOTE]
This field shows the brief machine-readable reason for the event
+
.. `message`
+
[NOTE]
This field shows the human-readable event message
+
. In the `Search` bar type the following in order to filter only the messages that contain field `kuberenetes.event`
+
[source, text]
----
_exists_:kubernetes.event
----
. Click `Save`
.. Enter name `Kubernetes Events`
.. Check checkbox 'Save as a new search'
.. Click `Save`


=== Correlate events via Timelion

In this section you will learn how to use Timelion feature of Kibana. This is a new Kibana application added in Kibana 5.x (included in OpenShift v3.11).

. Switch to `Timelion` tab of Kibana
. In the query bar search of all logs from project `lab1` where the application runs:
[source,text]
+
----
.es(index='project.lab1*')
----
+
. You should see chart of log volume from the project `lab1`
. In the query bar search of all events on the cluster:
[source,text]
+
----
.es(index='.operations*', q='_exists_:kubernetes.event')
----
. You should see chart of Kubernetes events volume from the cluster
. Combine both queries together in the query bar:
[source,text]
+
----
.es(index='project.lab1*'), .es(index='.operations*', q='_exists_:kubernetes.event')
----
+
. Click `Save`
.. Choose `Save current expression as Kibana dashboard panel`
.. Enter the name `Correlated events and logs`
.. Click `Save`

=== Create Dashboard for event and log correlation

We will create a dashboard that will display the Timelion visualization and saved searches created on previous steps.

. Click Dashboard tab
.. Click `+`
.. Click `add`
.. On the `Visualization` tab select the `Correlated events and logs` visualization you created in Timelion
.. On the `Search` tab select `Kubernetes Events` and `pod logs` saved searches you saved previously
.. Drag and resize visualizations and searches so that they fit the screen nicely.
.. Click `Save`
.. Enter name `Event Correlation`
.. Click `Save`

Now you have the dashboards that correlates Kubernetes Events and logs from various pods.

[NOTE]
It is necessary to use `Timelion` for correlation, because Kubernetes Events reside in `.operations*` index pattern, while application logs belong to one of `project.*` indices. Regular Kibana widgets can only work on the same index pattern.

=== Delete image stream and validate that it is reflected in the Dashboard

In this section you will trigger Kubernetes events and make sure they are visible in the dashboard.

. In the `Event Correlation` dashboard type the following query to limit display only to events related to `lab1` namespace or to logs produced in `lab1` namespace.
+
[source, text]
----
kubernetes.event.involvedObject.namespace:lab1 OR kubernetes.namespace_name:lab1
----
+
. Delete ImageStream TODO
. Check that events are properly reflected.

== Forwarding logs to different cluster

=== Get the certificate from destination cluster

TODO

=== Change fluentd config on the source cluster

TODO

== Cleanup

Now that you have finished this lab, please take a moment to clean up the environment for the next set of students.

. Log into the OpenShift Cluster 1 (https://{cluster_master}).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Delete the Jenkins Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-jenkins
----
+
. Delete the Development Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-rhte-app-dev
----
+
. Delete the Production Project on Cluster 1
+
[source,bash]
----
oc delete project ${GUID}-rhte-app
----
+
. Log into the OpenShift Cluster 2 (https://{cluster2_master}).
+
[source,bash]
----
oc login -u <OpenTLC User Name> -p <OpenTLC Password> <CLUSTER_URL>
----
+
. Delete the Production Project on Cluster 2
+
[source,bash]
----
oc delete project ${GUID}-rhte-app
----

Congratulations! You finished this lab.
